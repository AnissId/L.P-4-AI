# -*- coding: utf-8 -*-
"""Lab phase 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WjWTCNz_5HgqhyC-ftBg6zwF1si4UGi6
"""

import pandas as pd
df = pd.read_csv('/content/IMDB Dataset.csv')
df.head()

df.info()

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates()

df['sentiment'].unique()

df['sentiment'].value_counts()

Sentiment_classes = {'negative': 0, 'positive': 1 }
df['sentiment'] = df['sentiment'].map(Sentiment_classes)
df

import string
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.corpus import wordnet
from nltk import pos_tag
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

import re
def clean_text(text):
    # Remove words starting with '@'
    text = re.sub(r'@\w+', '', text)
    # Remove words starting with 'https://'
    text = re.sub(r'https://\S+', '', text)
    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()
    # Ensure the text ends with a period
    text = re.sub(r'#','',text)
    if not text.endswith('.'):
        text += '.'
    return text
df['Clean_review'] = df['review'].apply(clean_text)
df

import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
def preprocess_text(text):
    # Convert the text to lowercase
    text = text.lower()
    # Clean the text
    text = clean_text(text)
    # Tokenize the text
    tokens = word_tokenize(text)
    # Remove punctuation
    tokens = [word for word in tokens if word.isalnum()]  # Only keep alphanumeric tokens
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    # Initialize lemmatizer
    lemmatizer = WordNetLemmatizer()
    # Lemmatize the tokens
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    # Reconstruct the text from tokens
    text = ' '.join(tokens)
    return text, tokens
df['preprocess_text']=df['Clean_review'].apply(preprocess_text)
df

df['tokens'] = df['preprocess_text'].apply(lambda x: x[1])
df

from sklearn.feature_extraction.text import TfidfVectorizer

# CountVectorizer object for unigrams and bigrams
vectorizer = TfidfVectorizer()

# Extract the preprocessed text
preprocessed_texts = df['preprocess_text'].apply(lambda x: x[0])

# Fit and transform the preprocessed text data
X = vectorizer.fit_transform(preprocessed_texts)
y = df['sentiment']

print(X.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 40)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

NB_clf = MultinomialNB()
NB_clf.fit(X_train, y_train)

NB_pred = NB_clf.predict(X_test)

print('NB Accuracy', accuracy_score(y_test, NB_pred))
print("NB Classification Report:\n", classification_report(y_test, NB_pred))

# Decision Tree classifier
DT_clf = DecisionTreeClassifier()
DT_clf.fit(X_train, y_train)

# Predict
DT_pred = DT_clf.predict(X_test)

# Evaluate
print("Decision Tree Accuracy:", accuracy_score(y_test, DT_pred))
print("Decision Tree Classification Report:\n", classification_report(y_test, DT_pred))

# Hp T Naive Bayes
from sklearn.model_selection import GridSearchCV

# the parameter grid for Naive Bayes
param_grid_nb = {
    'alpha': [5.0, 6.0, 7.0],
    'fit_prior': [True, False],
    'class_prior': [None,[0.4, 0.6]]
}

nb_model = MultinomialNB()

# GridSearchCV
grid_search_nb = GridSearchCV(estimator=nb_model, param_grid=param_grid_nb, cv=5, scoring='accuracy')

# Fit GridSearchCV
grid_search_nb.fit(X_train, y_train)

print("Best Parameters for Naive Bayes:", grid_search_nb.best_params_)
print("Best Score for Naive Bayes:", grid_search_nb.best_score_)

# the parameter grid for Decision Tree
param_grid_dt = {
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'criterion': ['gini']
}
dt_model = DecisionTreeClassifier()

# GridSearchCV
grid_search_dt = GridSearchCV(estimator=dt_model, param_grid=param_grid_dt, cv=4, verbose=2, scoring='accuracy')

# Fit GridSearchCV
grid_search_dt.fit(X_train, y_train)

print("Best Parameters for Decision Tree:", grid_search_dt.best_params_)
print("Best Score for Decision Tree:", grid_search_dt.best_score_)